{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the models performed similarly, around .5 model accuracy. The training accuracy was much better, with random forrest and gradient boosting at 1 and SVM and logistic regression around .7. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I handled missing values using dropna to get rid of them. I encoded categorical data using onehot because my data is nominal. I removed outliers using robustscaler and standardized my data using standardscaler. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performed relatively poorly on SVM and logistic regression testing, possibly showing too much bias\n",
    "\n",
    "Perhaps too much variance on random forest and gradient boosting because performed well on training set and poorly on testing set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Model accuracy: 0.4810126582278481\n",
      "Gradient Boosting - Model accuracy: 0.4936708860759494\n",
      "SVM - Model accuracy: 0.46835443037974683\n",
      "Logistic Regression - Model accuracy: 0.4810126582278481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/willmarquart/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "df = pd.read_csv('student-mat.csv')\n",
    "\n",
    "df = df.dropna() \n",
    "\n",
    "X = df.drop('Walc', axis=1)  \n",
    "y = df['Walc']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('outlier_remover', RobustScaler(with_centering=False)),  \n",
    "    ('scaler', StandardScaler())  \n",
    "])\n",
    "\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'Logistic Regression': LogisticRegression()\n",
    "}\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    model_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    accuracy = model_pipeline.score(X_test, y_test)\n",
    "    print(f'{name} - Model accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Training accuracy: 1.0, Testing accuracy: 0.4810126582278481\n",
      "Gradient Boosting - Training accuracy: 1.0, Testing accuracy: 0.4936708860759494\n",
      "SVM - Training accuracy: 0.7183544303797469, Testing accuracy: 0.46835443037974683\n",
      "Logistic Regression - Training accuracy: 0.6550632911392406, Testing accuracy: 0.4810126582278481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/willmarquart/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for name, model in models.items():\n",
    "    model_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "    y_train_pred = model_pipeline.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    \n",
    "  \n",
    "    y_test_pred = model_pipeline.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f'{name} - Training accuracy: {train_accuracy}, Testing accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.4407968437886879\n",
      "RMSE: 0.9198895915493684\n",
      "MAE: 0.7731037381329114\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "model = LinearRegression()\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', model)\n",
    "])\n",
    "\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f'R-squared: {r_squared}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
